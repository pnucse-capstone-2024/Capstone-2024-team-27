{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from ./new_model_inst-summ/unsloth.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Model\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  13:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 128255\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Model\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: PAD token        = 128255 '<|reserved_special_token_250|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4403.49 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '128255', 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'Model', 'general.type': 'model', 'general.size_label': '8.0B', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "model_path='./new_model_inst-summ'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = Llama(\n",
    "    model_path='./new_model_inst-summ/unsloth.Q4_K_M.gguf', #다운로드받은 모델의 위치\n",
    "    n_ctx=2048,\n",
    "    n_gpu_layers=33        # Number of model layers to offload to GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8561\n",
      "8561\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('./news_summ.csv', encoding='UTF-8')\n",
    "\n",
    "sampled_df = df.sample(frac=0.2, random_state=42)\n",
    "\n",
    "news = list(sampled_df['news'])\n",
    "summ = list(sampled_df['summary'])\n",
    "\n",
    "print(len(news))\n",
    "print(len(summ))\n",
    "\n",
    "random_indices = random.sample(range(8561), 200)\n",
    "\n",
    "sample_news = [news[i] for i in random_indices]\n",
    "sample_summ = [summ[i] for i in random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1129: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.97 ms /    38 runs   (    0.05 ms per token, 19328.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     554.97 ms /   805 tokens (    0.69 ms per token,  1450.52 tokens per second)\n",
      "llama_print_timings:        eval time =     636.20 ms /    37 runs   (   17.19 ms per token,    58.16 tokens per second)\n",
      "llama_print_timings:       total time =    1207.29 ms /   842 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.27 ms /    44 runs   (    0.05 ms per token, 19400.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     292.84 ms /   468 tokens (    0.63 ms per token,  1598.12 tokens per second)\n",
      "llama_print_timings:        eval time =     727.55 ms /    43 runs   (   16.92 ms per token,    59.10 tokens per second)\n",
      "llama_print_timings:       total time =    1038.37 ms /   511 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.59 ms /    31 runs   (    0.05 ms per token, 19521.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     616.11 ms /   983 tokens (    0.63 ms per token,  1595.49 tokens per second)\n",
      "llama_print_timings:        eval time =     524.41 ms /    30 runs   (   17.48 ms per token,    57.21 tokens per second)\n",
      "llama_print_timings:       total time =    1153.82 ms /  1013 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.51 ms /    87 runs   (    0.05 ms per token, 19299.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     300.55 ms /   510 tokens (    0.59 ms per token,  1696.92 tokens per second)\n",
      "llama_print_timings:        eval time =    1459.28 ms /    86 runs   (   16.97 ms per token,    58.93 tokens per second)\n",
      "llama_print_timings:       total time =    1796.94 ms /   596 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /    42 runs   (    0.05 ms per token, 19266.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.64 ms /   944 tokens (    0.61 ms per token,  1634.23 tokens per second)\n",
      "llama_print_timings:        eval time =     714.42 ms /    41 runs   (   17.42 ms per token,    57.39 tokens per second)\n",
      "llama_print_timings:       total time =    1310.68 ms /   985 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.75 ms /    54 runs   (    0.05 ms per token, 19657.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     382.95 ms /   634 tokens (    0.60 ms per token,  1655.55 tokens per second)\n",
      "llama_print_timings:        eval time =     906.92 ms /    53 runs   (   17.11 ms per token,    58.44 tokens per second)\n",
      "llama_print_timings:       total time =    1312.31 ms /   687 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.21 ms /    62 runs   (    0.05 ms per token, 19338.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     366.80 ms /   607 tokens (    0.60 ms per token,  1654.88 tokens per second)\n",
      "llama_print_timings:        eval time =    1043.67 ms /    61 runs   (   17.11 ms per token,    58.45 tokens per second)\n",
      "llama_print_timings:       total time =    1435.44 ms /   668 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.44 ms /    47 runs   (    0.05 ms per token, 19262.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     357.83 ms /   579 tokens (    0.62 ms per token,  1618.06 tokens per second)\n",
      "llama_print_timings:        eval time =     785.15 ms /    46 runs   (   17.07 ms per token,    58.59 tokens per second)\n",
      "llama_print_timings:       total time =    1161.34 ms /   625 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.71 ms /   109 runs   (    0.05 ms per token, 19102.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     377.61 ms /   624 tokens (    0.61 ms per token,  1652.51 tokens per second)\n",
      "llama_print_timings:        eval time =    1852.22 ms /   108 runs   (   17.15 ms per token,    58.31 tokens per second)\n",
      "llama_print_timings:       total time =    2275.58 ms /   732 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.79 ms /    93 runs   (    0.05 ms per token, 19423.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     292.65 ms /   467 tokens (    0.63 ms per token,  1595.75 tokens per second)\n",
      "llama_print_timings:        eval time =    1560.56 ms /    92 runs   (   16.96 ms per token,    58.95 tokens per second)\n",
      "llama_print_timings:       total time =    1890.00 ms /   559 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.93 ms /    77 runs   (    0.05 ms per token, 19582.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     367.34 ms /   600 tokens (    0.61 ms per token,  1633.38 tokens per second)\n",
      "llama_print_timings:        eval time =    1300.85 ms /    76 runs   (   17.12 ms per token,    58.42 tokens per second)\n",
      "llama_print_timings:       total time =    1698.48 ms /   676 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.56 ms /    89 runs   (    0.05 ms per token, 19513.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     272.13 ms /   456 tokens (    0.60 ms per token,  1675.69 tokens per second)\n",
      "llama_print_timings:        eval time =    1495.64 ms /    88 runs   (   17.00 ms per token,    58.84 tokens per second)\n",
      "llama_print_timings:       total time =    1803.14 ms /   544 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.95 ms /    38 runs   (    0.05 ms per token, 19507.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     246.06 ms /   418 tokens (    0.59 ms per token,  1698.77 tokens per second)\n",
      "llama_print_timings:        eval time =     638.54 ms /    37 runs   (   17.26 ms per token,    57.94 tokens per second)\n",
      "llama_print_timings:       total time =     899.81 ms /   455 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.73 ms /    53 runs   (    0.05 ms per token, 19449.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     241.70 ms /   396 tokens (    0.61 ms per token,  1638.41 tokens per second)\n",
      "llama_print_timings:        eval time =     897.20 ms /    52 runs   (   17.25 ms per token,    57.96 tokens per second)\n",
      "llama_print_timings:       total time =    1159.85 ms /   448 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       7.57 ms /   147 runs   (    0.05 ms per token, 19411.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     375.79 ms /   614 tokens (    0.61 ms per token,  1633.90 tokens per second)\n",
      "llama_print_timings:        eval time =    2506.04 ms /   146 runs   (   17.16 ms per token,    58.26 tokens per second)\n",
      "llama_print_timings:       total time =    2943.82 ms /   760 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.36 ms /    65 runs   (    0.05 ms per token, 19339.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     349.53 ms /   562 tokens (    0.62 ms per token,  1607.86 tokens per second)\n",
      "llama_print_timings:        eval time =    1092.67 ms /    64 runs   (   17.07 ms per token,    58.57 tokens per second)\n",
      "llama_print_timings:       total time =    1467.27 ms /   626 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.88 ms /    56 runs   (    0.05 ms per token, 19478.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     344.70 ms /   558 tokens (    0.62 ms per token,  1618.79 tokens per second)\n",
      "llama_print_timings:        eval time =     939.24 ms /    55 runs   (   17.08 ms per token,    58.56 tokens per second)\n",
      "llama_print_timings:       total time =    1305.53 ms /   613 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.17 ms /   100 runs   (    0.05 ms per token, 19361.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     359.58 ms /   579 tokens (    0.62 ms per token,  1610.22 tokens per second)\n",
      "llama_print_timings:        eval time =    1695.45 ms /    99 runs   (   17.13 ms per token,    58.39 tokens per second)\n",
      "llama_print_timings:       total time =    2096.28 ms /   678 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.39 ms /    66 runs   (    0.05 ms per token, 19463.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     520.26 ms /   848 tokens (    0.61 ms per token,  1629.96 tokens per second)\n",
      "llama_print_timings:        eval time =    1130.44 ms /    65 runs   (   17.39 ms per token,    57.50 tokens per second)\n",
      "llama_print_timings:       total time =    1679.97 ms /   913 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.17 ms /    80 runs   (    0.05 ms per token, 19161.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     683.46 ms /  1081 tokens (    0.63 ms per token,  1581.67 tokens per second)\n",
      "llama_print_timings:        eval time =    1395.85 ms /    79 runs   (   17.67 ms per token,    56.60 tokens per second)\n",
      "llama_print_timings:       total time =    2114.86 ms /  1160 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.49 ms /    88 runs   (    0.05 ms per token, 19586.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     379.10 ms /   621 tokens (    0.61 ms per token,  1638.09 tokens per second)\n",
      "llama_print_timings:        eval time =    1493.33 ms /    87 runs   (   17.16 ms per token,    58.26 tokens per second)\n",
      "llama_print_timings:       total time =    1908.13 ms /   708 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.52 ms /    68 runs   (    0.05 ms per token, 19329.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     667.01 ms /  1036 tokens (    0.64 ms per token,  1553.19 tokens per second)\n",
      "llama_print_timings:        eval time =    1179.57 ms /    67 runs   (   17.61 ms per token,    56.80 tokens per second)\n",
      "llama_print_timings:       total time =    1875.53 ms /  1103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.75 ms /    54 runs   (    0.05 ms per token, 19622.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     408.78 ms /   648 tokens (    0.63 ms per token,  1585.19 tokens per second)\n",
      "llama_print_timings:        eval time =     910.46 ms /    53 runs   (   17.18 ms per token,    58.21 tokens per second)\n",
      "llama_print_timings:       total time =    1340.64 ms /   701 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.95 ms /    38 runs   (    0.05 ms per token, 19457.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     466.82 ms /   768 tokens (    0.61 ms per token,  1645.17 tokens per second)\n",
      "llama_print_timings:        eval time =     639.00 ms /    37 runs   (   17.27 ms per token,    57.90 tokens per second)\n",
      "llama_print_timings:       total time =    1121.11 ms /   805 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.05 ms /    79 runs   (    0.05 ms per token, 19525.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     379.02 ms /   621 tokens (    0.61 ms per token,  1638.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1339.32 ms /    78 runs   (   17.17 ms per token,    58.24 tokens per second)\n",
      "llama_print_timings:       total time =    1750.58 ms /   699 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.52 ms /    69 runs   (    0.05 ms per token, 19613.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     297.71 ms /   479 tokens (    0.62 ms per token,  1608.94 tokens per second)\n",
      "llama_print_timings:        eval time =    1155.38 ms /    68 runs   (   16.99 ms per token,    58.85 tokens per second)\n",
      "llama_print_timings:       total time =    1480.96 ms /   547 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.06 ms /    40 runs   (    0.05 ms per token, 19417.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     334.22 ms /   522 tokens (    0.64 ms per token,  1561.87 tokens per second)\n",
      "llama_print_timings:        eval time =     663.81 ms /    39 runs   (   17.02 ms per token,    58.75 tokens per second)\n",
      "llama_print_timings:       total time =    1014.10 ms /   561 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.76 ms /    32 runs   (    0.06 ms per token, 18161.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     353.82 ms /   573 tokens (    0.62 ms per token,  1619.49 tokens per second)\n",
      "llama_print_timings:        eval time =     529.60 ms /    31 runs   (   17.08 ms per token,    58.53 tokens per second)\n",
      "llama_print_timings:       total time =     895.39 ms /   604 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.88 ms /    95 runs   (    0.05 ms per token, 19471.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     335.35 ms /   524 tokens (    0.64 ms per token,  1562.56 tokens per second)\n",
      "llama_print_timings:        eval time =    1603.37 ms /    94 runs   (   17.06 ms per token,    58.63 tokens per second)\n",
      "llama_print_timings:       total time =    1977.15 ms /   618 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.71 ms /   111 runs   (    0.05 ms per token, 19436.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     445.40 ms /   718 tokens (    0.62 ms per token,  1612.04 tokens per second)\n",
      "llama_print_timings:        eval time =    1899.18 ms /   110 runs   (   17.27 ms per token,    57.92 tokens per second)\n",
      "llama_print_timings:       total time =    2391.57 ms /   828 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.25 ms /    44 runs   (    0.05 ms per token, 19581.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.98 ms /   457 tokens (    0.60 ms per token,  1661.93 tokens per second)\n",
      "llama_print_timings:        eval time =     733.89 ms /    43 runs   (   17.07 ms per token,    58.59 tokens per second)\n",
      "llama_print_timings:       total time =    1025.36 ms /   500 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.92 ms /    37 runs   (    0.05 ms per token, 19270.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.03 ms /   452 tokens (    0.61 ms per token,  1649.43 tokens per second)\n",
      "llama_print_timings:        eval time =     616.60 ms /    36 runs   (   17.13 ms per token,    58.38 tokens per second)\n",
      "llama_print_timings:       total time =     904.80 ms /   488 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.93 ms /    57 runs   (    0.05 ms per token, 19487.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     306.13 ms /   510 tokens (    0.60 ms per token,  1665.94 tokens per second)\n",
      "llama_print_timings:        eval time =     954.08 ms /    56 runs   (   17.04 ms per token,    58.70 tokens per second)\n",
      "llama_print_timings:       total time =    1281.99 ms /   566 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.24 ms /    82 runs   (    0.05 ms per token, 19335.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     578.68 ms /   906 tokens (    0.64 ms per token,  1565.63 tokens per second)\n",
      "llama_print_timings:        eval time =    1416.66 ms /    81 runs   (   17.49 ms per token,    57.18 tokens per second)\n",
      "llama_print_timings:       total time =    2028.96 ms /   987 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.29 ms /    64 runs   (    0.05 ms per token, 19476.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     588.24 ms /   943 tokens (    0.62 ms per token,  1603.09 tokens per second)\n",
      "llama_print_timings:        eval time =    1104.10 ms /    63 runs   (   17.53 ms per token,    57.06 tokens per second)\n",
      "llama_print_timings:       total time =    1717.10 ms /  1006 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.84 ms /    55 runs   (    0.05 ms per token, 19338.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     297.68 ms /   470 tokens (    0.63 ms per token,  1578.87 tokens per second)\n",
      "llama_print_timings:        eval time =     918.28 ms /    54 runs   (   17.01 ms per token,    58.81 tokens per second)\n",
      "llama_print_timings:       total time =    1236.51 ms /   524 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.44 ms /    67 runs   (    0.05 ms per token, 19505.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     362.44 ms /   580 tokens (    0.62 ms per token,  1600.24 tokens per second)\n",
      "llama_print_timings:        eval time =    1131.67 ms /    66 runs   (   17.15 ms per token,    58.32 tokens per second)\n",
      "llama_print_timings:       total time =    1520.60 ms /   646 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.66 ms /    32 runs   (    0.05 ms per token, 19253.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     354.10 ms /   563 tokens (    0.63 ms per token,  1589.94 tokens per second)\n",
      "llama_print_timings:        eval time =     529.31 ms /    31 runs   (   17.07 ms per token,    58.57 tokens per second)\n",
      "llama_print_timings:       total time =     894.84 ms /   594 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.36 ms /    46 runs   (    0.05 ms per token, 19516.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     410.29 ms /   643 tokens (    0.64 ms per token,  1567.18 tokens per second)\n",
      "llama_print_timings:        eval time =     773.77 ms /    45 runs   (   17.19 ms per token,    58.16 tokens per second)\n",
      "llama_print_timings:       total time =    1200.91 ms /   688 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.54 ms /    30 runs   (    0.05 ms per token, 19430.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     551.60 ms /   890 tokens (    0.62 ms per token,  1613.49 tokens per second)\n",
      "llama_print_timings:        eval time =     506.29 ms /    29 runs   (   17.46 ms per token,    57.28 tokens per second)\n",
      "llama_print_timings:       total time =    1069.32 ms /   919 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.61 ms /   110 runs   (    0.05 ms per token, 19621.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     447.53 ms /   720 tokens (    0.62 ms per token,  1608.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1883.53 ms /   109 runs   (   17.28 ms per token,    57.87 tokens per second)\n",
      "llama_print_timings:       total time =    2375.99 ms /   829 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =      12.16 ms /   240 runs   (    0.05 ms per token, 19741.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     469.97 ms /   768 tokens (    0.61 ms per token,  1634.16 tokens per second)\n",
      "llama_print_timings:        eval time =    4162.60 ms /   239 runs   (   17.42 ms per token,    57.42 tokens per second)\n",
      "llama_print_timings:       total time =    4739.72 ms /  1007 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.26 ms /    64 runs   (    0.05 ms per token, 19619.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.80 ms /   457 tokens (    0.60 ms per token,  1657.00 tokens per second)\n",
      "llama_print_timings:        eval time =    1074.34 ms /    63 runs   (   17.05 ms per token,    58.64 tokens per second)\n",
      "llama_print_timings:       total time =    1375.85 ms /   520 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.95 ms /    77 runs   (    0.05 ms per token, 19503.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     469.20 ms /   762 tokens (    0.62 ms per token,  1624.04 tokens per second)\n",
      "llama_print_timings:        eval time =    1315.67 ms /    76 runs   (   17.31 ms per token,    57.77 tokens per second)\n",
      "llama_print_timings:       total time =    1816.46 ms /   838 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.03 ms /    99 runs   (    0.05 ms per token, 19670.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     302.18 ms /   492 tokens (    0.61 ms per token,  1628.16 tokens per second)\n",
      "llama_print_timings:        eval time =    1670.58 ms /    98 runs   (   17.05 ms per token,    58.66 tokens per second)\n",
      "llama_print_timings:       total time =    2013.89 ms /   590 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       6.20 ms /   122 runs   (    0.05 ms per token, 19667.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     276.50 ms /   461 tokens (    0.60 ms per token,  1667.29 tokens per second)\n",
      "llama_print_timings:        eval time =    2062.87 ms /   121 runs   (   17.05 ms per token,    58.66 tokens per second)\n",
      "llama_print_timings:       total time =    2390.20 ms /   582 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.66 ms /   109 runs   (    0.05 ms per token, 19244.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     355.53 ms /   570 tokens (    0.62 ms per token,  1603.24 tokens per second)\n",
      "llama_print_timings:        eval time =    1853.36 ms /   108 runs   (   17.16 ms per token,    58.27 tokens per second)\n",
      "llama_print_timings:       total time =    2253.67 ms /   678 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.64 ms /    32 runs   (    0.05 ms per token, 19500.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     517.27 ms /   818 tokens (    0.63 ms per token,  1581.39 tokens per second)\n",
      "llama_print_timings:        eval time =     538.03 ms /    31 runs   (   17.36 ms per token,    57.62 tokens per second)\n",
      "llama_print_timings:       total time =    1067.74 ms /   849 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.21 ms /    82 runs   (    0.05 ms per token, 19468.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     380.73 ms /   616 tokens (    0.62 ms per token,  1617.96 tokens per second)\n",
      "llama_print_timings:        eval time =    1393.14 ms /    81 runs   (   17.20 ms per token,    58.14 tokens per second)\n",
      "llama_print_timings:       total time =    1807.60 ms /   697 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       7.03 ms /   138 runs   (    0.05 ms per token, 19644.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     305.75 ms /   505 tokens (    0.61 ms per token,  1651.70 tokens per second)\n",
      "llama_print_timings:        eval time =    2341.32 ms /   137 runs   (   17.09 ms per token,    58.51 tokens per second)\n",
      "llama_print_timings:       total time =    2705.01 ms /   642 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.05 ms /    79 runs   (    0.05 ms per token, 19491.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     305.36 ms /   502 tokens (    0.61 ms per token,  1643.96 tokens per second)\n",
      "llama_print_timings:        eval time =    1329.87 ms /    78 runs   (   17.05 ms per token,    58.65 tokens per second)\n",
      "llama_print_timings:       total time =    1666.85 ms /   580 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.82 ms /    94 runs   (    0.05 ms per token, 19506.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     299.65 ms /   479 tokens (    0.63 ms per token,  1598.54 tokens per second)\n",
      "llama_print_timings:        eval time =    1584.33 ms /    93 runs   (   17.04 ms per token,    58.70 tokens per second)\n",
      "llama_print_timings:       total time =    1921.75 ms /   572 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.98 ms /    96 runs   (    0.05 ms per token, 19261.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     578.50 ms /   900 tokens (    0.64 ms per token,  1555.76 tokens per second)\n",
      "llama_print_timings:        eval time =    1661.89 ms /    95 runs   (   17.49 ms per token,    57.16 tokens per second)\n",
      "llama_print_timings:       total time =    2279.99 ms /   995 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.81 ms /    55 runs   (    0.05 ms per token, 19593.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     253.15 ms /   448 tokens (    0.57 ms per token,  1769.67 tokens per second)\n",
      "llama_print_timings:        eval time =     924.72 ms /    54 runs   (   17.12 ms per token,    58.40 tokens per second)\n",
      "llama_print_timings:       total time =    1198.91 ms /   502 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.48 ms /    67 runs   (    0.05 ms per token, 19280.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     276.82 ms /   465 tokens (    0.60 ms per token,  1679.80 tokens per second)\n",
      "llama_print_timings:        eval time =    1123.27 ms /    66 runs   (   17.02 ms per token,    58.76 tokens per second)\n",
      "llama_print_timings:       total time =    1426.31 ms /   531 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.35 ms /    84 runs   (    0.05 ms per token, 19314.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     371.64 ms /   596 tokens (    0.62 ms per token,  1603.72 tokens per second)\n",
      "llama_print_timings:        eval time =    1426.17 ms /    83 runs   (   17.18 ms per token,    58.20 tokens per second)\n",
      "llama_print_timings:       total time =    1832.10 ms /   679 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.88 ms /    95 runs   (    0.05 ms per token, 19467.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     417.18 ms /   672 tokens (    0.62 ms per token,  1610.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1620.91 ms /    94 runs   (   17.24 ms per token,    57.99 tokens per second)\n",
      "llama_print_timings:       total time =    2077.13 ms /   766 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.89 ms /    37 runs   (    0.05 ms per token, 19535.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     300.86 ms /   486 tokens (    0.62 ms per token,  1615.40 tokens per second)\n",
      "llama_print_timings:        eval time =     613.06 ms /    36 runs   (   17.03 ms per token,    58.72 tokens per second)\n",
      "llama_print_timings:       total time =     928.11 ms /   522 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.90 ms /    76 runs   (    0.05 ms per token, 19492.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     390.42 ms /   634 tokens (    0.62 ms per token,  1623.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1290.94 ms /    75 runs   (   17.21 ms per token,    58.10 tokens per second)\n",
      "llama_print_timings:       total time =    1712.11 ms /   709 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.68 ms /    52 runs   (    0.05 ms per token, 19374.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     299.99 ms /   480 tokens (    0.62 ms per token,  1600.05 tokens per second)\n",
      "llama_print_timings:        eval time =     867.97 ms /    51 runs   (   17.02 ms per token,    58.76 tokens per second)\n",
      "llama_print_timings:       total time =    1187.85 ms /   531 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.98 ms /    38 runs   (    0.05 ms per token, 19221.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.34 ms /   365 tokens (    0.59 ms per token,  1702.90 tokens per second)\n",
      "llama_print_timings:        eval time =     639.50 ms /    37 runs   (   17.28 ms per token,    57.86 tokens per second)\n",
      "llama_print_timings:       total time =     869.47 ms /   402 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =      44.50 ms /   863 runs   (    0.05 ms per token, 19393.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     243.42 ms /   386 tokens (    0.63 ms per token,  1585.72 tokens per second)\n",
      "llama_print_timings:        eval time =   14984.66 ms /   862 runs   (   17.38 ms per token,    57.53 tokens per second)\n",
      "llama_print_timings:       total time =   15818.06 ms /  1248 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.35 ms /    65 runs   (    0.05 ms per token, 19397.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     250.43 ms /   426 tokens (    0.59 ms per token,  1701.05 tokens per second)\n",
      "llama_print_timings:        eval time =    1101.47 ms /    64 runs   (   17.21 ms per token,    58.10 tokens per second)\n",
      "llama_print_timings:       total time =    1377.51 ms /   490 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    42 runs   (    0.05 ms per token, 19328.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     301.43 ms /   487 tokens (    0.62 ms per token,  1615.66 tokens per second)\n",
      "llama_print_timings:        eval time =     698.17 ms /    41 runs   (   17.03 ms per token,    58.73 tokens per second)\n",
      "llama_print_timings:       total time =    1015.68 ms /   528 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.81 ms /    35 runs   (    0.05 ms per token, 19337.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     448.07 ms /   718 tokens (    0.62 ms per token,  1602.41 tokens per second)\n",
      "llama_print_timings:        eval time =     586.95 ms /    34 runs   (   17.26 ms per token,    57.93 tokens per second)\n",
      "llama_print_timings:       total time =    1049.08 ms /   752 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.70 ms /    92 runs   (    0.05 ms per token, 19578.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     336.29 ms /   522 tokens (    0.64 ms per token,  1552.21 tokens per second)\n",
      "llama_print_timings:        eval time =    1555.05 ms /    91 runs   (   17.09 ms per token,    58.52 tokens per second)\n",
      "llama_print_timings:       total time =    1929.10 ms /   613 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.73 ms /    53 runs   (    0.05 ms per token, 19406.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     337.89 ms /   517 tokens (    0.65 ms per token,  1530.10 tokens per second)\n",
      "llama_print_timings:        eval time =     887.44 ms /    52 runs   (   17.07 ms per token,    58.60 tokens per second)\n",
      "llama_print_timings:       total time =    1246.20 ms /   569 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /   102 runs   (    0.05 ms per token, 19514.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     302.88 ms /   495 tokens (    0.61 ms per token,  1634.31 tokens per second)\n",
      "llama_print_timings:        eval time =    1723.24 ms /   101 runs   (   17.06 ms per token,    58.61 tokens per second)\n",
      "llama_print_timings:       total time =    2067.83 ms /   596 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.25 ms /    83 runs   (    0.05 ms per token, 19547.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     243.00 ms /   386 tokens (    0.63 ms per token,  1588.46 tokens per second)\n",
      "llama_print_timings:        eval time =    1419.68 ms /    82 runs   (   17.31 ms per token,    57.76 tokens per second)\n",
      "llama_print_timings:       total time =    1695.06 ms /   468 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.34 ms /    65 runs   (    0.05 ms per token, 19472.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     428.21 ms /   675 tokens (    0.63 ms per token,  1576.33 tokens per second)\n",
      "llama_print_timings:        eval time =    1103.69 ms /    64 runs   (   17.25 ms per token,    57.99 tokens per second)\n",
      "llama_print_timings:       total time =    1557.65 ms /   739 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       6.10 ms /   118 runs   (    0.05 ms per token, 19356.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     521.91 ms /   830 tokens (    0.63 ms per token,  1590.32 tokens per second)\n",
      "llama_print_timings:        eval time =    2040.31 ms /   117 runs   (   17.44 ms per token,    57.34 tokens per second)\n",
      "llama_print_timings:       total time =    2611.84 ms /   947 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.04 ms /    40 runs   (    0.05 ms per token, 19607.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     494.80 ms /   784 tokens (    0.63 ms per token,  1584.48 tokens per second)\n",
      "llama_print_timings:        eval time =     675.53 ms /    39 runs   (   17.32 ms per token,    57.73 tokens per second)\n",
      "llama_print_timings:       total time =    1186.49 ms /   823 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.41 ms /    47 runs   (    0.05 ms per token, 19534.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.09 ms /   451 tokens (    0.61 ms per token,  1639.46 tokens per second)\n",
      "llama_print_timings:        eval time =     787.44 ms /    46 runs   (   17.12 ms per token,    58.42 tokens per second)\n",
      "llama_print_timings:       total time =    1080.67 ms /   497 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.33 ms /    26 runs   (    0.05 ms per token, 19490.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     336.67 ms /   522 tokens (    0.64 ms per token,  1550.50 tokens per second)\n",
      "llama_print_timings:        eval time =     426.18 ms /    25 runs   (   17.05 ms per token,    58.66 tokens per second)\n",
      "llama_print_timings:       total time =     773.31 ms /   547 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.95 ms /    58 runs   (    0.05 ms per token, 19647.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     381.05 ms /   612 tokens (    0.62 ms per token,  1606.10 tokens per second)\n",
      "llama_print_timings:        eval time =     979.87 ms /    57 runs   (   17.19 ms per token,    58.17 tokens per second)\n",
      "llama_print_timings:       total time =    1383.49 ms /   669 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.45 ms /    66 runs   (    0.05 ms per token, 19152.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     299.11 ms /   477 tokens (    0.63 ms per token,  1594.72 tokens per second)\n",
      "llama_print_timings:        eval time =    1107.29 ms /    65 runs   (   17.04 ms per token,    58.70 tokens per second)\n",
      "llama_print_timings:       total time =    1432.36 ms /   542 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.13 ms /    61 runs   (    0.05 ms per token, 19482.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     248.72 ms /   412 tokens (    0.60 ms per token,  1656.50 tokens per second)\n",
      "llama_print_timings:        eval time =    1037.90 ms /    60 runs   (   17.30 ms per token,    57.81 tokens per second)\n",
      "llama_print_timings:       total time =    1310.32 ms /   472 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.86 ms /   114 runs   (    0.05 ms per token, 19460.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     305.13 ms /   500 tokens (    0.61 ms per token,  1638.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1930.06 ms /   113 runs   (   17.08 ms per token,    58.55 tokens per second)\n",
      "llama_print_timings:       total time =    2282.21 ms /   613 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.04 ms /    59 runs   (    0.05 ms per token, 19420.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     299.52 ms /   477 tokens (    0.63 ms per token,  1592.53 tokens per second)\n",
      "llama_print_timings:        eval time =     987.63 ms /    58 runs   (   17.03 ms per token,    58.73 tokens per second)\n",
      "llama_print_timings:       total time =    1310.87 ms /   535 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.61 ms /    71 runs   (    0.05 ms per token, 19651.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     383.40 ms /   624 tokens (    0.61 ms per token,  1627.54 tokens per second)\n",
      "llama_print_timings:        eval time =    1204.32 ms /    70 runs   (   17.20 ms per token,    58.12 tokens per second)\n",
      "llama_print_timings:       total time =    1615.94 ms /   694 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.65 ms /    71 runs   (    0.05 ms per token, 19436.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     413.61 ms /   656 tokens (    0.63 ms per token,  1586.03 tokens per second)\n",
      "llama_print_timings:        eval time =    1206.31 ms /    70 runs   (   17.23 ms per token,    58.03 tokens per second)\n",
      "llama_print_timings:       total time =    1648.35 ms /   726 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.73 ms /    33 runs   (    0.05 ms per token, 19119.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     344.42 ms /   545 tokens (    0.63 ms per token,  1582.38 tokens per second)\n",
      "llama_print_timings:        eval time =     546.24 ms /    32 runs   (   17.07 ms per token,    58.58 tokens per second)\n",
      "llama_print_timings:       total time =     903.43 ms /   577 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.88 ms /    56 runs   (    0.05 ms per token, 19430.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     433.03 ms /   696 tokens (    0.62 ms per token,  1607.28 tokens per second)\n",
      "llama_print_timings:        eval time =     949.81 ms /    55 runs   (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:       total time =    1404.56 ms /   751 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.11 ms /   100 runs   (    0.05 ms per token, 19580.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     633.78 ms /   994 tokens (    0.64 ms per token,  1568.37 tokens per second)\n",
      "llama_print_timings:        eval time =    1746.66 ms /    99 runs   (   17.64 ms per token,    56.68 tokens per second)\n",
      "llama_print_timings:       total time =    2421.76 ms /  1093 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.35 ms /    66 runs   (    0.05 ms per token, 19707.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     358.46 ms /   576 tokens (    0.62 ms per token,  1606.88 tokens per second)\n",
      "llama_print_timings:        eval time =    1115.88 ms /    65 runs   (   17.17 ms per token,    58.25 tokens per second)\n",
      "llama_print_timings:       total time =    1501.08 ms /   641 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.24 ms /    44 runs   (    0.05 ms per token, 19642.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     306.33 ms /   502 tokens (    0.61 ms per token,  1638.76 tokens per second)\n",
      "llama_print_timings:        eval time =     733.61 ms /    43 runs   (   17.06 ms per token,    58.61 tokens per second)\n",
      "llama_print_timings:       total time =    1057.19 ms /   545 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       7.23 ms /   139 runs   (    0.05 ms per token, 19220.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     582.18 ms /   903 tokens (    0.64 ms per token,  1551.06 tokens per second)\n",
      "llama_print_timings:        eval time =    2422.82 ms /   138 runs   (   17.56 ms per token,    56.96 tokens per second)\n",
      "llama_print_timings:       total time =    3064.33 ms /  1041 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       7.86 ms /   152 runs   (    0.05 ms per token, 19331.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     342.26 ms /   544 tokens (    0.63 ms per token,  1589.45 tokens per second)\n",
      "llama_print_timings:        eval time =    2593.25 ms /   151 runs   (   17.17 ms per token,    58.23 tokens per second)\n",
      "llama_print_timings:       total time =    3000.18 ms /   695 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.57 ms /    50 runs   (    0.05 ms per token, 19470.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     365.71 ms /   586 tokens (    0.62 ms per token,  1602.37 tokens per second)\n",
      "llama_print_timings:        eval time =     841.67 ms /    49 runs   (   17.18 ms per token,    58.22 tokens per second)\n",
      "llama_print_timings:       total time =    1227.07 ms /   635 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.97 ms /    77 runs   (    0.05 ms per token, 19390.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     453.87 ms /   732 tokens (    0.62 ms per token,  1612.79 tokens per second)\n",
      "llama_print_timings:        eval time =    1314.96 ms /    76 runs   (   17.30 ms per token,    57.80 tokens per second)\n",
      "llama_print_timings:       total time =    1800.22 ms /   808 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.85 ms /    75 runs   (    0.05 ms per token, 19460.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     471.53 ms /   762 tokens (    0.62 ms per token,  1616.02 tokens per second)\n",
      "llama_print_timings:        eval time =    1282.51 ms /    74 runs   (   17.33 ms per token,    57.70 tokens per second)\n",
      "llama_print_timings:       total time =    1784.27 ms /   836 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.56 ms /    70 runs   (    0.05 ms per token, 19640.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     278.50 ms /   463 tokens (    0.60 ms per token,  1662.45 tokens per second)\n",
      "llama_print_timings:        eval time =    1176.50 ms /    69 runs   (   17.05 ms per token,    58.65 tokens per second)\n",
      "llama_print_timings:       total time =    1482.76 ms /   532 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.67 ms /    52 runs   (    0.05 ms per token, 19497.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     414.92 ms /   656 tokens (    0.63 ms per token,  1581.02 tokens per second)\n",
      "llama_print_timings:        eval time =     879.13 ms /    51 runs   (   17.24 ms per token,    58.01 tokens per second)\n",
      "llama_print_timings:       total time =    1314.56 ms /   707 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.43 ms /    67 runs   (    0.05 ms per token, 19556.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     546.31 ms /   852 tokens (    0.64 ms per token,  1559.57 tokens per second)\n",
      "llama_print_timings:        eval time =    1152.44 ms /    66 runs   (   17.46 ms per token,    57.27 tokens per second)\n",
      "llama_print_timings:       total time =    1725.41 ms /   918 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.68 ms /    91 runs   (    0.05 ms per token, 19436.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     349.64 ms /   560 tokens (    0.62 ms per token,  1601.62 tokens per second)\n",
      "llama_print_timings:        eval time =    1544.31 ms /    90 runs   (   17.16 ms per token,    58.28 tokens per second)\n",
      "llama_print_timings:       total time =    1930.56 ms /   650 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.22 ms /    62 runs   (    0.05 ms per token, 19230.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     551.25 ms /   877 tokens (    0.63 ms per token,  1590.92 tokens per second)\n",
      "llama_print_timings:        eval time =    1066.23 ms /    61 runs   (   17.48 ms per token,    57.21 tokens per second)\n",
      "llama_print_timings:       total time =    1642.04 ms /   938 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.97 ms /    77 runs   (    0.05 ms per token, 19395.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     619.45 ms /   963 tokens (    0.64 ms per token,  1554.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1336.92 ms /    76 runs   (   17.59 ms per token,    56.85 tokens per second)\n",
      "llama_print_timings:       total time =    1987.73 ms /  1039 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.54 ms /    49 runs   (    0.05 ms per token, 19298.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     468.29 ms /   748 tokens (    0.63 ms per token,  1597.29 tokens per second)\n",
      "llama_print_timings:        eval time =     830.43 ms /    48 runs   (   17.30 ms per token,    57.80 tokens per second)\n",
      "llama_print_timings:       total time =    1318.23 ms /   796 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.00 ms /    78 runs   (    0.05 ms per token, 19504.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     453.24 ms /   730 tokens (    0.62 ms per token,  1610.64 tokens per second)\n",
      "llama_print_timings:        eval time =    1331.99 ms /    77 runs   (   17.30 ms per token,    57.81 tokens per second)\n",
      "llama_print_timings:       total time =    1816.67 ms /   807 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.03 ms /    98 runs   (    0.05 ms per token, 19494.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     618.29 ms /   962 tokens (    0.64 ms per token,  1555.90 tokens per second)\n",
      "llama_print_timings:        eval time =    1707.47 ms /    97 runs   (   17.60 ms per token,    56.81 tokens per second)\n",
      "llama_print_timings:       total time =    2366.82 ms /  1059 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.81 ms /    93 runs   (    0.05 ms per token, 19342.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     467.46 ms /   746 tokens (    0.63 ms per token,  1595.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1593.61 ms /    92 runs   (   17.32 ms per token,    57.73 tokens per second)\n",
      "llama_print_timings:       total time =    2098.77 ms /   838 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.70 ms /    53 runs   (    0.05 ms per token, 19636.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     300.70 ms /   479 tokens (    0.63 ms per token,  1592.94 tokens per second)\n",
      "llama_print_timings:        eval time =     886.57 ms /    52 runs   (   17.05 ms per token,    58.65 tokens per second)\n",
      "llama_print_timings:       total time =    1207.51 ms /   531 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.93 ms /    57 runs   (    0.05 ms per token, 19427.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     518.34 ms /   810 tokens (    0.64 ms per token,  1562.68 tokens per second)\n",
      "llama_print_timings:        eval time =     974.05 ms /    56 runs   (   17.39 ms per token,    57.49 tokens per second)\n",
      "llama_print_timings:       total time =    1515.01 ms /   866 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.55 ms /   109 runs   (    0.05 ms per token, 19643.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     301.20 ms /   479 tokens (    0.63 ms per token,  1590.30 tokens per second)\n",
      "llama_print_timings:        eval time =    1841.68 ms /   108 runs   (   17.05 ms per token,    58.64 tokens per second)\n",
      "llama_print_timings:       total time =    2185.82 ms /   587 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.02 ms /    60 runs   (    0.05 ms per token, 19874.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     277.21 ms /   457 tokens (    0.61 ms per token,  1648.56 tokens per second)\n",
      "llama_print_timings:        eval time =    1007.95 ms /    59 runs   (   17.08 ms per token,    58.53 tokens per second)\n",
      "llama_print_timings:       total time =    1308.26 ms /   516 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.99 ms /    78 runs   (    0.05 ms per token, 19539.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     344.83 ms /   545 tokens (    0.63 ms per token,  1580.50 tokens per second)\n",
      "llama_print_timings:        eval time =    1319.00 ms /    77 runs   (   17.13 ms per token,    58.38 tokens per second)\n",
      "llama_print_timings:       total time =    1694.96 ms /   622 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       6.81 ms /   132 runs   (    0.05 ms per token, 19394.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     357.44 ms /   568 tokens (    0.63 ms per token,  1589.08 tokens per second)\n",
      "llama_print_timings:        eval time =    2252.13 ms /   131 runs   (   17.19 ms per token,    58.17 tokens per second)\n",
      "llama_print_timings:       total time =    2664.83 ms /   699 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.53 ms /    69 runs   (    0.05 ms per token, 19524.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     365.40 ms /   584 tokens (    0.63 ms per token,  1598.24 tokens per second)\n",
      "llama_print_timings:        eval time =    1168.37 ms /    68 runs   (   17.18 ms per token,    58.20 tokens per second)\n",
      "llama_print_timings:       total time =    1560.93 ms /   652 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.75 ms /    73 runs   (    0.05 ms per token, 19451.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     276.10 ms /   450 tokens (    0.61 ms per token,  1629.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1231.58 ms /    72 runs   (   17.11 ms per token,    58.46 tokens per second)\n",
      "llama_print_timings:       total time =    1536.48 ms /   522 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.25 ms /   103 runs   (    0.05 ms per token, 19607.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     414.78 ms /   654 tokens (    0.63 ms per token,  1576.73 tokens per second)\n",
      "llama_print_timings:        eval time =    1759.98 ms /   102 runs   (   17.25 ms per token,    57.96 tokens per second)\n",
      "llama_print_timings:       total time =    2216.48 ms /   756 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.44 ms /    28 runs   (    0.05 ms per token, 19417.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     365.88 ms /   586 tokens (    0.62 ms per token,  1601.63 tokens per second)\n",
      "llama_print_timings:        eval time =     463.45 ms /    27 runs   (   17.16 ms per token,    58.26 tokens per second)\n",
      "llama_print_timings:       total time =     840.32 ms /   613 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.08 ms /    98 runs   (    0.05 ms per token, 19279.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     357.31 ms /   567 tokens (    0.63 ms per token,  1586.86 tokens per second)\n",
      "llama_print_timings:        eval time =    1665.73 ms /    97 runs   (   17.17 ms per token,    58.23 tokens per second)\n",
      "llama_print_timings:       total time =    2062.83 ms /   664 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.22 ms /    62 runs   (    0.05 ms per token, 19266.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     549.24 ms /   868 tokens (    0.63 ms per token,  1580.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1066.18 ms /    61 runs   (   17.48 ms per token,    57.21 tokens per second)\n",
      "llama_print_timings:       total time =    1640.31 ms /   929 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.55 ms /   108 runs   (    0.05 ms per token, 19445.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     341.80 ms /   538 tokens (    0.64 ms per token,  1574.03 tokens per second)\n",
      "llama_print_timings:        eval time =    1833.62 ms /   107 runs   (   17.14 ms per token,    58.35 tokens per second)\n",
      "llama_print_timings:       total time =    2219.52 ms /   645 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.28 ms /    44 runs   (    0.05 ms per token, 19272.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     338.34 ms /   523 tokens (    0.65 ms per token,  1545.80 tokens per second)\n",
      "llama_print_timings:        eval time =     734.32 ms /    43 runs   (   17.08 ms per token,    58.56 tokens per second)\n",
      "llama_print_timings:       total time =    1089.47 ms /   566 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.64 ms /    52 runs   (    0.05 ms per token, 19734.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     337.84 ms /   522 tokens (    0.65 ms per token,  1545.11 tokens per second)\n",
      "llama_print_timings:        eval time =     871.08 ms /    51 runs   (   17.08 ms per token,    58.55 tokens per second)\n",
      "llama_print_timings:       total time =    1228.64 ms /   573 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.53 ms /    49 runs   (    0.05 ms per token, 19375.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     303.46 ms /   492 tokens (    0.62 ms per token,  1621.28 tokens per second)\n",
      "llama_print_timings:        eval time =     818.87 ms /    48 runs   (   17.06 ms per token,    58.62 tokens per second)\n",
      "llama_print_timings:       total time =    1141.42 ms /   540 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.87 ms /    56 runs   (    0.05 ms per token, 19546.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     252.46 ms /   435 tokens (    0.58 ms per token,  1723.02 tokens per second)\n",
      "llama_print_timings:        eval time =     946.69 ms /    55 runs   (   17.21 ms per token,    58.10 tokens per second)\n",
      "llama_print_timings:       total time =    1220.99 ms /   490 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.01 ms /    78 runs   (    0.05 ms per token, 19446.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     373.20 ms /   595 tokens (    0.63 ms per token,  1594.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1324.75 ms /    77 runs   (   17.20 ms per token,    58.12 tokens per second)\n",
      "llama_print_timings:       total time =    1729.83 ms /   672 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.89 ms /    75 runs   (    0.05 ms per token, 19295.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     254.47 ms /   444 tokens (    0.57 ms per token,  1744.82 tokens per second)\n",
      "llama_print_timings:        eval time =    1267.90 ms /    74 runs   (   17.13 ms per token,    58.36 tokens per second)\n",
      "llama_print_timings:       total time =    1551.99 ms /   518 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.09 ms /    78 runs   (    0.05 ms per token, 19047.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     390.98 ms /   626 tokens (    0.62 ms per token,  1601.11 tokens per second)\n",
      "llama_print_timings:        eval time =    1326.72 ms /    77 runs   (   17.23 ms per token,    58.04 tokens per second)\n",
      "llama_print_timings:       total time =    1749.00 ms /   703 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.75 ms /    34 runs   (    0.05 ms per token, 19384.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     470.49 ms /   756 tokens (    0.62 ms per token,  1606.83 tokens per second)\n",
      "llama_print_timings:        eval time =     571.19 ms /    33 runs   (   17.31 ms per token,    57.77 tokens per second)\n",
      "llama_print_timings:       total time =    1054.87 ms /   789 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /    43 runs   (    0.05 ms per token, 19256.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     326.80 ms /   514 tokens (    0.64 ms per token,  1572.82 tokens per second)\n",
      "llama_print_timings:        eval time =     716.93 ms /    42 runs   (   17.07 ms per token,    58.58 tokens per second)\n",
      "llama_print_timings:       total time =    1060.53 ms /   556 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.69 ms /    52 runs   (    0.05 ms per token, 19345.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     301.32 ms /   481 tokens (    0.63 ms per token,  1596.29 tokens per second)\n",
      "llama_print_timings:        eval time =     869.15 ms /    51 runs   (   17.04 ms per token,    58.68 tokens per second)\n",
      "llama_print_timings:       total time =    1190.41 ms /   532 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.84 ms /    55 runs   (    0.05 ms per token, 19359.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     363.95 ms /   577 tokens (    0.63 ms per token,  1585.39 tokens per second)\n",
      "llama_print_timings:        eval time =     927.03 ms /    54 runs   (   17.17 ms per token,    58.25 tokens per second)\n",
      "llama_print_timings:       total time =    1313.21 ms /   631 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.35 ms /    45 runs   (    0.05 ms per token, 19189.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     435.36 ms /   699 tokens (    0.62 ms per token,  1605.58 tokens per second)\n",
      "llama_print_timings:        eval time =     760.14 ms /    44 runs   (   17.28 ms per token,    57.88 tokens per second)\n",
      "llama_print_timings:       total time =    1213.51 ms /   743 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.64 ms /    90 runs   (    0.05 ms per token, 19413.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     618.96 ms /   964 tokens (    0.64 ms per token,  1557.46 tokens per second)\n",
      "llama_print_timings:        eval time =    1567.63 ms /    89 runs   (   17.61 ms per token,    56.77 tokens per second)\n",
      "llama_print_timings:       total time =    2224.15 ms /  1053 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.59 ms /    31 runs   (    0.05 ms per token, 19472.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     366.60 ms /   589 tokens (    0.62 ms per token,  1606.64 tokens per second)\n",
      "llama_print_timings:        eval time =     515.39 ms /    30 runs   (   17.18 ms per token,    58.21 tokens per second)\n",
      "llama_print_timings:       total time =     893.79 ms /   619 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    93 runs   (    0.05 ms per token, 19443.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     418.21 ms /   668 tokens (    0.63 ms per token,  1597.28 tokens per second)\n",
      "llama_print_timings:        eval time =    1588.72 ms /    92 runs   (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:       total time =    2044.38 ms /   760 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.77 ms /    54 runs   (    0.05 ms per token, 19466.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     468.41 ms /   751 tokens (    0.62 ms per token,  1603.29 tokens per second)\n",
      "llama_print_timings:        eval time =     917.65 ms /    53 runs   (   17.31 ms per token,    57.76 tokens per second)\n",
      "llama_print_timings:       total time =    1407.18 ms /   804 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.40 ms /   105 runs   (    0.05 ms per token, 19451.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     250.31 ms /   419 tokens (    0.60 ms per token,  1673.89 tokens per second)\n",
      "llama_print_timings:        eval time =    1786.65 ms /   104 runs   (   17.18 ms per token,    58.21 tokens per second)\n",
      "llama_print_timings:       total time =    2079.48 ms /   523 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.39 ms /    66 runs   (    0.05 ms per token, 19451.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     521.35 ms /   819 tokens (    0.64 ms per token,  1570.92 tokens per second)\n",
      "llama_print_timings:        eval time =    1131.95 ms /    65 runs   (   17.41 ms per token,    57.42 tokens per second)\n",
      "llama_print_timings:       total time =    1680.13 ms /   884 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.33 ms /    65 runs   (    0.05 ms per token, 19513.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     303.42 ms /   489 tokens (    0.62 ms per token,  1611.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1091.73 ms /    64 runs   (   17.06 ms per token,    58.62 tokens per second)\n",
      "llama_print_timings:       total time =    1421.06 ms /   553 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.79 ms /    74 runs   (    0.05 ms per token, 19525.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     302.19 ms /   485 tokens (    0.62 ms per token,  1604.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1245.08 ms /    73 runs   (   17.06 ms per token,    58.63 tokens per second)\n",
      "llama_print_timings:       total time =    1576.95 ms /   558 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /    42 runs   (    0.05 ms per token, 19266.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     382.18 ms /   612 tokens (    0.62 ms per token,  1601.36 tokens per second)\n",
      "llama_print_timings:        eval time =     705.20 ms /    41 runs   (   17.20 ms per token,    58.14 tokens per second)\n",
      "llama_print_timings:       total time =    1104.08 ms /   653 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.61 ms /   109 runs   (    0.05 ms per token, 19446.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     349.76 ms /   560 tokens (    0.62 ms per token,  1601.10 tokens per second)\n",
      "llama_print_timings:        eval time =    1854.90 ms /   108 runs   (   17.18 ms per token,    58.22 tokens per second)\n",
      "llama_print_timings:       total time =    2249.91 ms /   668 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.89 ms /    36 runs   (    0.05 ms per token, 19057.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     252.49 ms /   434 tokens (    0.58 ms per token,  1718.88 tokens per second)\n",
      "llama_print_timings:        eval time =     606.68 ms /    35 runs   (   17.33 ms per token,    57.69 tokens per second)\n",
      "llama_print_timings:       total time =     872.92 ms /   469 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.99 ms /    98 runs   (    0.05 ms per token, 19631.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     435.47 ms /   700 tokens (    0.62 ms per token,  1607.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1677.43 ms /    97 runs   (   17.29 ms per token,    57.83 tokens per second)\n",
      "llama_print_timings:       total time =    2153.17 ms /   797 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.82 ms /    55 runs   (    0.05 ms per token, 19496.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     366.45 ms /   588 tokens (    0.62 ms per token,  1604.57 tokens per second)\n",
      "llama_print_timings:        eval time =     928.23 ms /    54 runs   (   17.19 ms per token,    58.18 tokens per second)\n",
      "llama_print_timings:       total time =    1316.65 ms /   642 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.25 ms /    82 runs   (    0.05 ms per token, 19303.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     373.66 ms /   597 tokens (    0.63 ms per token,  1597.70 tokens per second)\n",
      "llama_print_timings:        eval time =    1394.16 ms /    81 runs   (   17.21 ms per token,    58.10 tokens per second)\n",
      "llama_print_timings:       total time =    1800.70 ms /   678 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.30 ms /    44 runs   (    0.05 ms per token, 19155.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     348.76 ms /   554 tokens (    0.63 ms per token,  1588.48 tokens per second)\n",
      "llama_print_timings:        eval time =     735.86 ms /    43 runs   (   17.11 ms per token,    58.44 tokens per second)\n",
      "llama_print_timings:       total time =    1102.10 ms /   597 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.30 ms /    83 runs   (    0.05 ms per token, 19306.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     679.33 ms /  1059 tokens (    0.64 ms per token,  1558.88 tokens per second)\n",
      "llama_print_timings:        eval time =    1452.88 ms /    82 runs   (   17.72 ms per token,    56.44 tokens per second)\n",
      "llama_print_timings:       total time =    2167.18 ms /  1141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.04 ms /    59 runs   (    0.05 ms per token, 19376.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     307.41 ms /   507 tokens (    0.61 ms per token,  1649.27 tokens per second)\n",
      "llama_print_timings:        eval time =     990.01 ms /    58 runs   (   17.07 ms per token,    58.59 tokens per second)\n",
      "llama_print_timings:       total time =    1320.02 ms /   565 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.12 ms /    41 runs   (    0.05 ms per token, 19330.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     450.90 ms /   721 tokens (    0.63 ms per token,  1599.03 tokens per second)\n",
      "llama_print_timings:        eval time =     691.45 ms /    40 runs   (   17.29 ms per token,    57.85 tokens per second)\n",
      "llama_print_timings:       total time =    1159.05 ms /   761 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.27 ms /    83 runs   (    0.05 ms per token, 19433.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     307.11 ms /   505 tokens (    0.61 ms per token,  1644.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1400.48 ms /    82 runs   (   17.08 ms per token,    58.55 tokens per second)\n",
      "llama_print_timings:       total time =    1741.10 ms /   587 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.69 ms /    33 runs   (    0.05 ms per token, 19480.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     306.92 ms /   502 tokens (    0.61 ms per token,  1635.60 tokens per second)\n",
      "llama_print_timings:        eval time =     545.78 ms /    32 runs   (   17.06 ms per token,    58.63 tokens per second)\n",
      "llama_print_timings:       total time =     864.72 ms /   534 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.16 ms /    61 runs   (    0.05 ms per token, 19297.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     699.81 ms /  1089 tokens (    0.64 ms per token,  1556.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1064.55 ms /    60 runs   (   17.74 ms per token,    56.36 tokens per second)\n",
      "llama_print_timings:       total time =    1789.55 ms /  1149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.21 ms /   102 runs   (    0.05 ms per token, 19573.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     752.27 ms /  1156 tokens (    0.65 ms per token,  1536.67 tokens per second)\n",
      "llama_print_timings:        eval time =    1801.05 ms /   101 runs   (   17.83 ms per token,    56.08 tokens per second)\n",
      "llama_print_timings:       total time =    2595.80 ms /  1257 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.24 ms /    82 runs   (    0.05 ms per token, 19353.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     430.23 ms /   676 tokens (    0.64 ms per token,  1571.24 tokens per second)\n",
      "llama_print_timings:        eval time =    1398.79 ms /    81 runs   (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:       total time =    1861.97 ms /   757 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.95 ms /    77 runs   (    0.05 ms per token, 19473.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     246.49 ms /   397 tokens (    0.62 ms per token,  1610.63 tokens per second)\n",
      "llama_print_timings:        eval time =    1315.91 ms /    76 runs   (   17.31 ms per token,    57.75 tokens per second)\n",
      "llama_print_timings:       total time =    1592.69 ms /   473 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.99 ms /    97 runs   (    0.05 ms per token, 19423.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     593.37 ms /   948 tokens (    0.63 ms per token,  1597.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1688.85 ms /    96 runs   (   17.59 ms per token,    56.84 tokens per second)\n",
      "llama_print_timings:       total time =    2322.58 ms /  1044 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.95 ms /    76 runs   (    0.05 ms per token, 19264.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     383.02 ms /   617 tokens (    0.62 ms per token,  1610.88 tokens per second)\n",
      "llama_print_timings:        eval time =    1291.50 ms /    75 runs   (   17.22 ms per token,    58.07 tokens per second)\n",
      "llama_print_timings:       total time =    1704.62 ms /   692 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.47 ms /    86 runs   (    0.05 ms per token, 19243.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     278.57 ms /   464 tokens (    0.60 ms per token,  1665.63 tokens per second)\n",
      "llama_print_timings:        eval time =    1449.76 ms /    85 runs   (   17.06 ms per token,    58.63 tokens per second)\n",
      "llama_print_timings:       total time =    1762.46 ms /   549 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.07 ms /    60 runs   (    0.05 ms per token, 19569.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     384.99 ms /   623 tokens (    0.62 ms per token,  1618.23 tokens per second)\n",
      "llama_print_timings:        eval time =    1016.18 ms /    59 runs   (   17.22 ms per token,    58.06 tokens per second)\n",
      "llama_print_timings:       total time =    1425.36 ms /   682 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.60 ms /    51 runs   (    0.05 ms per token, 19600.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     467.06 ms /   741 tokens (    0.63 ms per token,  1586.53 tokens per second)\n",
      "llama_print_timings:        eval time =     865.40 ms /    50 runs   (   17.31 ms per token,    57.78 tokens per second)\n",
      "llama_print_timings:       total time =    1352.34 ms /   791 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.88 ms /    94 runs   (    0.05 ms per token, 19270.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     339.66 ms /   526 tokens (    0.65 ms per token,  1548.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1592.00 ms /    93 runs   (   17.12 ms per token,    58.42 tokens per second)\n",
      "llama_print_timings:       total time =    1969.45 ms /   619 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.90 ms /    95 runs   (    0.05 ms per token, 19379.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     621.69 ms /   974 tokens (    0.64 ms per token,  1566.71 tokens per second)\n",
      "llama_print_timings:        eval time =    1657.24 ms /    94 runs   (   17.63 ms per token,    56.72 tokens per second)\n",
      "llama_print_timings:       total time =    2318.54 ms /  1068 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.24 ms /    63 runs   (    0.05 ms per token, 19462.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     299.12 ms /   470 tokens (    0.64 ms per token,  1571.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1056.70 ms /    62 runs   (   17.04 ms per token,    58.67 tokens per second)\n",
      "llama_print_timings:       total time =    1381.41 ms /   532 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.51 ms /    49 runs   (    0.05 ms per token, 19545.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     391.72 ms /   629 tokens (    0.62 ms per token,  1605.76 tokens per second)\n",
      "llama_print_timings:        eval time =     826.59 ms /    48 runs   (   17.22 ms per token,    58.07 tokens per second)\n",
      "llama_print_timings:       total time =    1237.90 ms /   677 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /   110 runs   (    0.05 ms per token, 19349.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     434.98 ms /   697 tokens (    0.62 ms per token,  1602.37 tokens per second)\n",
      "llama_print_timings:        eval time =    1885.01 ms /   109 runs   (   17.29 ms per token,    57.82 tokens per second)\n",
      "llama_print_timings:       total time =    2366.36 ms /   806 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.64 ms /    70 runs   (    0.05 ms per token, 19214.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     392.44 ms /   633 tokens (    0.62 ms per token,  1612.99 tokens per second)\n",
      "llama_print_timings:        eval time =    1189.32 ms /    69 runs   (   17.24 ms per token,    58.02 tokens per second)\n",
      "llama_print_timings:       total time =    1609.72 ms /   702 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.38 ms /    66 runs   (    0.05 ms per token, 19538.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     367.38 ms /   590 tokens (    0.62 ms per token,  1605.98 tokens per second)\n",
      "llama_print_timings:        eval time =    1117.53 ms /    65 runs   (   17.19 ms per token,    58.16 tokens per second)\n",
      "llama_print_timings:       total time =    1511.32 ms /   655 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.66 ms /    32 runs   (    0.05 ms per token, 19323.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     453.81 ms /   729 tokens (    0.62 ms per token,  1606.38 tokens per second)\n",
      "llama_print_timings:        eval time =     535.84 ms /    31 runs   (   17.29 ms per token,    57.85 tokens per second)\n",
      "llama_print_timings:       total time =    1002.42 ms /   760 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    93 runs   (    0.05 ms per token, 19439.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     375.94 ms /   608 tokens (    0.62 ms per token,  1617.30 tokens per second)\n",
      "llama_print_timings:        eval time =    1584.40 ms /    92 runs   (   17.22 ms per token,    58.07 tokens per second)\n",
      "llama_print_timings:       total time =    1998.46 ms /   700 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.47 ms /   105 runs   (    0.05 ms per token, 19192.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     414.56 ms /   654 tokens (    0.63 ms per token,  1577.57 tokens per second)\n",
      "llama_print_timings:        eval time =    1795.48 ms /   104 runs   (   17.26 ms per token,    57.92 tokens per second)\n",
      "llama_print_timings:       total time =    2253.39 ms /   758 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /   110 runs   (    0.05 ms per token, 19338.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     277.40 ms /   458 tokens (    0.61 ms per token,  1651.06 tokens per second)\n",
      "llama_print_timings:        eval time =    1861.75 ms /   109 runs   (   17.08 ms per token,    58.55 tokens per second)\n",
      "llama_print_timings:       total time =    2184.27 ms /   567 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    42 runs   (    0.05 ms per token, 19354.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     392.05 ms /   631 tokens (    0.62 ms per token,  1609.51 tokens per second)\n",
      "llama_print_timings:        eval time =     706.36 ms /    41 runs   (   17.23 ms per token,    58.04 tokens per second)\n",
      "llama_print_timings:       total time =    1114.31 ms /   672 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /    43 runs   (    0.05 ms per token, 19457.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     431.74 ms /   684 tokens (    0.63 ms per token,  1584.30 tokens per second)\n",
      "llama_print_timings:        eval time =     725.77 ms /    42 runs   (   17.28 ms per token,    57.87 tokens per second)\n",
      "llama_print_timings:       total time =    1174.04 ms /   726 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.92 ms /    57 runs   (    0.05 ms per token, 19520.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     416.97 ms /   664 tokens (    0.63 ms per token,  1592.43 tokens per second)\n",
      "llama_print_timings:        eval time =     967.34 ms /    56 runs   (   17.27 ms per token,    57.89 tokens per second)\n",
      "llama_print_timings:       total time =    1406.54 ms /   720 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.74 ms /    53 runs   (    0.05 ms per token, 19336.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     469.55 ms /   752 tokens (    0.62 ms per token,  1601.55 tokens per second)\n",
      "llama_print_timings:        eval time =     900.97 ms /    52 runs   (   17.33 ms per token,    57.72 tokens per second)\n",
      "llama_print_timings:       total time =    1391.20 ms /   804 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.04 ms /    59 runs   (    0.05 ms per token, 19376.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     416.84 ms /   661 tokens (    0.63 ms per token,  1585.76 tokens per second)\n",
      "llama_print_timings:        eval time =    1001.39 ms /    58 runs   (   17.27 ms per token,    57.92 tokens per second)\n",
      "llama_print_timings:       total time =    1441.23 ms /   719 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.02 ms /    77 runs   (    0.05 ms per token, 19149.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     416.83 ms /   659 tokens (    0.63 ms per token,  1580.98 tokens per second)\n",
      "llama_print_timings:        eval time =    1313.54 ms /    76 runs   (   17.28 ms per token,    57.86 tokens per second)\n",
      "llama_print_timings:       total time =    1760.93 ms /   735 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       1.79 ms /    35 runs   (    0.05 ms per token, 19607.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     473.10 ms /   767 tokens (    0.62 ms per token,  1621.23 tokens per second)\n",
      "llama_print_timings:        eval time =     589.97 ms /    34 runs   (   17.35 ms per token,    57.63 tokens per second)\n",
      "llama_print_timings:       total time =    1076.05 ms /   801 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.31 ms /    64 runs   (    0.05 ms per token, 19335.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     632.75 ms /   991 tokens (    0.64 ms per token,  1566.18 tokens per second)\n",
      "llama_print_timings:        eval time =    1111.16 ms /    63 runs   (   17.64 ms per token,    56.70 tokens per second)\n",
      "llama_print_timings:       total time =    1770.14 ms /  1054 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.85 ms /    55 runs   (    0.05 ms per token, 19318.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     732.04 ms /  1149 tokens (    0.64 ms per token,  1569.58 tokens per second)\n",
      "llama_print_timings:        eval time =     961.46 ms /    54 runs   (   17.80 ms per token,    56.16 tokens per second)\n",
      "llama_print_timings:       total time =    1714.76 ms /  1203 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.85 ms /    55 runs   (    0.05 ms per token, 19284.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     308.83 ms /   511 tokens (    0.60 ms per token,  1654.61 tokens per second)\n",
      "llama_print_timings:        eval time =     922.06 ms /    54 runs   (   17.08 ms per token,    58.56 tokens per second)\n",
      "llama_print_timings:       total time =    1253.20 ms /   565 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /   104 runs   (    0.05 ms per token, 19745.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     412.66 ms /   643 tokens (    0.64 ms per token,  1558.19 tokens per second)\n",
      "llama_print_timings:        eval time =    1777.68 ms /   103 runs   (   17.26 ms per token,    57.94 tokens per second)\n",
      "llama_print_timings:       total time =    2233.14 ms /   746 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.10 ms /    60 runs   (    0.05 ms per token, 19379.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     384.39 ms /   623 tokens (    0.62 ms per token,  1620.73 tokens per second)\n",
      "llama_print_timings:        eval time =    1016.07 ms /    59 runs   (   17.22 ms per token,    58.07 tokens per second)\n",
      "llama_print_timings:       total time =    1424.55 ms /   682 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.17 ms /    81 runs   (    0.05 ms per token, 19410.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     356.93 ms /   568 tokens (    0.63 ms per token,  1591.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1373.66 ms /    80 runs   (   17.17 ms per token,    58.24 tokens per second)\n",
      "llama_print_timings:       total time =    1763.52 ms /   648 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.20 ms /    82 runs   (    0.05 ms per token, 19514.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     721.10 ms /  1129 tokens (    0.64 ms per token,  1565.65 tokens per second)\n",
      "llama_print_timings:        eval time =    1441.45 ms /    81 runs   (   17.80 ms per token,    56.19 tokens per second)\n",
      "llama_print_timings:       total time =    2197.28 ms /  1210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.09 ms /    60 runs   (    0.05 ms per token, 19392.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     468.07 ms /   745 tokens (    0.63 ms per token,  1591.65 tokens per second)\n",
      "llama_print_timings:        eval time =    1021.39 ms /    59 runs   (   17.31 ms per token,    57.76 tokens per second)\n",
      "llama_print_timings:       total time =    1513.52 ms /   804 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       8.02 ms /   156 runs   (    0.05 ms per token, 19446.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     304.55 ms /   494 tokens (    0.62 ms per token,  1622.09 tokens per second)\n",
      "llama_print_timings:        eval time =    2654.34 ms /   155 runs   (   17.12 ms per token,    58.39 tokens per second)\n",
      "llama_print_timings:       total time =    3025.46 ms /   649 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       7.49 ms /   147 runs   (    0.05 ms per token, 19634.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     302.01 ms /   482 tokens (    0.63 ms per token,  1595.98 tokens per second)\n",
      "llama_print_timings:        eval time =    2496.44 ms /   146 runs   (   17.10 ms per token,    58.48 tokens per second)\n",
      "llama_print_timings:       total time =    2860.69 ms /   628 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.20 ms /    43 runs   (    0.05 ms per token, 19527.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     303.77 ms /   491 tokens (    0.62 ms per token,  1616.33 tokens per second)\n",
      "llama_print_timings:        eval time =     716.60 ms /    42 runs   (   17.06 ms per token,    58.61 tokens per second)\n",
      "llama_print_timings:       total time =    1037.16 ms /   533 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       6.08 ms /   119 runs   (    0.05 ms per token, 19562.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     341.37 ms /   540 tokens (    0.63 ms per token,  1581.88 tokens per second)\n",
      "llama_print_timings:        eval time =    2024.57 ms /   118 runs   (   17.16 ms per token,    58.28 tokens per second)\n",
      "llama_print_timings:       total time =    2415.58 ms /   658 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.43 ms /    47 runs   (    0.05 ms per token, 19349.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     382.12 ms /   611 tokens (    0.63 ms per token,  1598.98 tokens per second)\n",
      "llama_print_timings:        eval time =     791.59 ms /    46 runs   (   17.21 ms per token,    58.11 tokens per second)\n",
      "llama_print_timings:       total time =    1191.84 ms /   657 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.44 ms /    47 runs   (    0.05 ms per token, 19293.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     587.35 ms /   917 tokens (    0.64 ms per token,  1561.26 tokens per second)\n",
      "llama_print_timings:        eval time =     806.08 ms /    46 runs   (   17.52 ms per token,    57.07 tokens per second)\n",
      "llama_print_timings:       total time =    1412.44 ms /   963 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.25 ms /    44 runs   (    0.05 ms per token, 19599.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     391.82 ms /   630 tokens (    0.62 ms per token,  1607.89 tokens per second)\n",
      "llama_print_timings:        eval time =     740.75 ms /    43 runs   (   17.23 ms per token,    58.05 tokens per second)\n",
      "llama_print_timings:       total time =    1149.27 ms /   673 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.03 ms /    40 runs   (    0.05 ms per token, 19714.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     520.08 ms /   815 tokens (    0.64 ms per token,  1567.07 tokens per second)\n",
      "llama_print_timings:        eval time =     678.27 ms /    39 runs   (   17.39 ms per token,    57.50 tokens per second)\n",
      "llama_print_timings:       total time =    1214.68 ms /   854 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.10 ms /    59 runs   (    0.05 ms per token, 19050.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     345.12 ms /   546 tokens (    0.63 ms per token,  1582.05 tokens per second)\n",
      "llama_print_timings:        eval time =     993.24 ms /    58 runs   (   17.12 ms per token,    58.40 tokens per second)\n",
      "llama_print_timings:       total time =    1362.17 ms /   604 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       6.43 ms /   126 runs   (    0.05 ms per token, 19589.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     358.94 ms /   575 tokens (    0.62 ms per token,  1601.94 tokens per second)\n",
      "llama_print_timings:        eval time =    2150.93 ms /   125 runs   (   17.21 ms per token,    58.11 tokens per second)\n",
      "llama_print_timings:       total time =    2563.27 ms /   700 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.72 ms /    92 runs   (    0.05 ms per token, 19503.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     301.92 ms /   481 tokens (    0.63 ms per token,  1593.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1552.82 ms /    91 runs   (   17.06 ms per token,    58.60 tokens per second)\n",
      "llama_print_timings:       total time =    1892.34 ms /   572 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.35 ms /    65 runs   (    0.05 ms per token, 19426.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     341.89 ms /   540 tokens (    0.63 ms per token,  1579.45 tokens per second)\n",
      "llama_print_timings:        eval time =    1095.15 ms /    64 runs   (   17.11 ms per token,    58.44 tokens per second)\n",
      "llama_print_timings:       total time =    1463.13 ms /   604 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.02 ms /    59 runs   (    0.05 ms per token, 19536.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     300.94 ms /   480 tokens (    0.63 ms per token,  1594.98 tokens per second)\n",
      "llama_print_timings:        eval time =     989.11 ms /    58 runs   (   17.05 ms per token,    58.64 tokens per second)\n",
      "llama_print_timings:       total time =    1312.93 ms /   538 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.41 ms /    66 runs   (    0.05 ms per token, 19354.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     468.47 ms /   747 tokens (    0.63 ms per token,  1594.55 tokens per second)\n",
      "llama_print_timings:        eval time =    1125.95 ms /    65 runs   (   17.32 ms per token,    57.73 tokens per second)\n",
      "llama_print_timings:       total time =    1621.08 ms /   812 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.13 ms /    61 runs   (    0.05 ms per token, 19488.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     497.64 ms /   783 tokens (    0.64 ms per token,  1573.42 tokens per second)\n",
      "llama_print_timings:        eval time =    1041.59 ms /    60 runs   (   17.36 ms per token,    57.60 tokens per second)\n",
      "llama_print_timings:       total time =    1563.83 ms /   843 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       4.17 ms /    81 runs   (    0.05 ms per token, 19415.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     250.00 ms /   415 tokens (    0.60 ms per token,  1660.03 tokens per second)\n",
      "llama_print_timings:        eval time =    1378.83 ms /    80 runs   (   17.24 ms per token,    58.02 tokens per second)\n",
      "llama_print_timings:       total time =    1660.91 ms /   495 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       2.63 ms /    51 runs   (    0.05 ms per token, 19362.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     357.01 ms /   566 tokens (    0.63 ms per token,  1585.40 tokens per second)\n",
      "llama_print_timings:        eval time =     857.30 ms /    50 runs   (   17.15 ms per token,    58.32 tokens per second)\n",
      "llama_print_timings:       total time =    1234.26 ms /   616 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.25 ms /    63 runs   (    0.05 ms per token, 19354.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     594.78 ms /   955 tokens (    0.62 ms per token,  1605.65 tokens per second)\n",
      "llama_print_timings:        eval time =    1090.36 ms /    62 runs   (   17.59 ms per token,    56.86 tokens per second)\n",
      "llama_print_timings:       total time =    1711.12 ms /  1017 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     351.83 ms\n",
      "llama_print_timings:      sample time =       3.55 ms /    69 runs   (    0.05 ms per token, 19425.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     418.79 ms /   666 tokens (    0.63 ms per token,  1590.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1173.80 ms /    68 runs   (   17.26 ms per token,    57.93 tokens per second)\n",
      "llama_print_timings:       total time =    1620.43 ms /   734 tokens\n"
     ]
    }
   ],
   "source": [
    "output_summ = []\n",
    "\n",
    "for text in sample_news:\n",
    "\n",
    "    PROMPT = \"정확한 챗봇으로서 상대방의 입력에 대해 요약을 하자. 모든 대답은 한국어(Korean)으로 대답해줘.\"\n",
    "    instruction = text\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    generation_kwargs = {\n",
    "        \"max_tokens\": 2048,\n",
    "        \"stop\": [\"<|eot_id|>\"],\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"echo\": True,  # 프롬프트를 출력에 포함합니다.\n",
    "    }\n",
    "\n",
    "    response_msg = model(prompt, **generation_kwargs)\n",
    "    summ         = response_msg['choices'][0]['text'][len(prompt):]\n",
    "\n",
    "    output_summ.append(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('summ.pkl', 'wb') as file1, \\\n",
    "    open('output_summ.pkl', 'wb') as file2:\n",
    "\n",
    "    pickle.dump(sample_summ, file1)\n",
    "    pickle.dump(output_summ, file2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
